{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji09xLvKqBBd"
      },
      "source": [
        "<p> ################################################################## </p>\n",
        "<p>#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#\n",
        "<p> # This source code is adopted from Tomaz and modified for NWMSU chatbot application  &nbsp; #</p>\n",
        "<p>#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#\n",
        "<p></p>\n",
        "<p></p>\n",
        "<p> ################################################################## </p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqgaHIIWAv0E"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8W6QNR3xzEO",
        "outputId": "c01479dc-3878-4e5d-f2bf-3f4af45082f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "[Errno 2] No such file or directory: 'gdrive/My Drive'\n",
            "/content/gdrive/My Drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "%cd gdrive/My Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5jQBSpUA1YJ"
      },
      "source": [
        "Requirements:\n",
        "\n",
        "1. **LangChain**: A framework for developing applications powered by language models.\n",
        "2. **LangChain-Community**: A collaborative space for sharing resources, tools, and discussions related to LangChain.\n",
        "3. **LangChain-OpenAI**: A LangChain extension providing integration with OpenAI's language models.\n",
        "4. **LangChain-Experimental**: A repository for experimental features and prototypes within the LangChain ecosystem.\n",
        "5. **Neo4j**: A graph database management system designed for handling and querying connected data.\n",
        "6. **tiktoken**: A library for tokenizing text, commonly used with language models to preprocess input and output.\n",
        "7. **yfiles_jupyter_graphs**: A library for creating and visualizing graphs and network diagrams in Jupyter notebooks.\n",
        "8. **Streamlit**: An open-source framework for building interactive, web-based applications using Python.\n",
        "9. **Localtunnel**: A tool that allows you to expose your local web server to the internet via a secure tunnel.\n",
        "10. **CTransformers** : A python wrapper for transfomer based models with essential configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "_e8mFtjUpsf-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10c6fde0-922a-4211-ff41-2c1494e25e60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet  langchain langchain-community langchain-openai langchain-experimental neo4j tiktoken yfiles_jupyter_graphs streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFTCrVRKSfls",
        "outputId": "f620ee26-72b5-4c50-dfe0-43e87c42bbd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ctransformers\n",
            "  Downloading ctransformers-0.2.27-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from ctransformers) (0.30.2)\n",
            "Requirement already satisfied: py-cpuinfo<10.0.0,>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from ctransformers) (9.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->ctransformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->ctransformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->ctransformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->ctransformers) (2025.1.31)\n",
            "Downloading ctransformers-0.2.27-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ctransformers\n",
            "Successfully installed ctransformers-0.2.27\n"
          ]
        }
      ],
      "source": [
        "pip install ctransformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQpoIlvoWU-Y",
        "outputId": "824e0964-6874-477f-855a-1ed3b2e10a54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: neo4j in /usr/local/lib/python3.11/dist-packages (5.28.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from neo4j) (2025.2)\n"
          ]
        }
      ],
      "source": [
        "pip install neo4j"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKRc3oenyXvO",
        "outputId": "1d29cbd5-4cb1-483f-d0f7-f0180b8b0909"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n",
            "up to date, audited 23 packages in 929ms\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n",
            "2 \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K"
          ]
        }
      ],
      "source": [
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQtND_fzFRNI"
      },
      "source": [
        "The provided code imports several components from different libraries, each serving a specific purpose. The `RunnableBranch`, `RunnableLambda`, `RunnableParallel`, and `RunnablePassthrough` from LangChain Core are utilities for creating complex workflows involving branching logic, function definitions, parallel execution, and straightforward data passing. For crafting prompts, `ChatPromptTemplate` and `PromptTemplate` are used to design structured inputs for language models.\n",
        "\n",
        "The `BaseModel` and `Field` from Pydantic assist in data validation and configuration, while typing utilities like `Tuple`, `List`, and `Optional` are used for type hinting in Python code. The message classes `AIMessage` and `HumanMessage` define interactions between an AI and a user. The `StrOutputParser` is employed for converting outputs into string format.\n",
        "\n",
        "The `os` module provides a way to interact with the operating system, and `Neo4jGraph` facilitates working with Neo4j databases. `TokenTextSplitter` helps in breaking down text into manageable tokens, essential for language processing. `ChatOpenAI` enables communication with OpenAI's chat models, and `LLMGraphTransformer` is used for transforming graph data with language models. For database interaction, `GraphDatabase` connects with Neo4j, and `GraphWidget` allows for the creation and visualization of graphs in Jupyter notebooks.\n",
        "\n",
        "Further, `Neo4jVector` supports storing and retrieving embeddings in Neo4j, while `OpenAIEmbeddings` provides tools for generating embeddings from OpenAI models. The `remove_lucene_chars` function cleans text by removing specific characters, ensuring data is properly formatted for Neo4j. Lastly, `ConfigurableField` allows for configurable data fields within a runnable, aiding in the customization of workflows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "n5jFoh3cqFfH"
      },
      "outputs": [],
      "source": [
        "# %%writefile main.py\n",
        "\n",
        "from langchain_core.runnables import (\n",
        "    RunnableBranch,\n",
        "    RunnableLambda,\n",
        "    RunnableParallel,\n",
        "    RunnablePassthrough,\n",
        ")\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.prompts.prompt import PromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from typing import Tuple, List, Optional\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import os\n",
        "from langchain_community.graphs import Neo4jGraph\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
        "from neo4j import GraphDatabase\n",
        "from yfiles_jupyter_graphs import GraphWidget\n",
        "from langchain_community.vectorstores import Neo4jVector\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars\n",
        "from langchain_core.runnables import ConfigurableField, RunnableParallel, RunnablePassthrough\n",
        "\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  from google.colab import output\n",
        "  output.enable_custom_widget_manager()\n",
        "except:\n",
        "  pass\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-n8hzmiPGDkr"
      },
      "source": [
        "These lines of code set environment variables for accessing the OpenAI API and a Neo4j database:\n",
        "\n",
        "- `os.environ[\"OPENAI_API_KEY\"] = \"sk-....\"` sets the API key for accessing OpenAI's services.\n",
        "- `os.environ[\"NEO4J_URI\"] = \"bolt+s://d......databases.neo4j.io:7687\"` specifies the URI for connecting to the Neo4j database.\n",
        "- `os.environ[\"NEO4J_USERNAME\"] = \"usr\"` sets the username for the Neo4j database connection.\n",
        "- `os.environ[\"NEO4J_PASSWORD\"] = \"pwd\"` sets the password for the Neo4j database connection.\n",
        "\n",
        "These environment variables are crucial for securely storing sensitive information required for connecting to external services."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "u-x0sDvkzApK"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"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\" # replace your OPEN_API key\n",
        "os.environ[\"NEO4J_URI\"] = \"neo4j+s://30dbcc45.databases.neo4j.io\" # NEO4J_URI\n",
        "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\" # NEO4J_USERNAME\n",
        "os.environ[\"NEO4J_PASSWORD\"] = \"8prKk8O5HAmILGnrvU2doJNH4OvGlpcIW8syj0tzgKg\" # NEO4J_PASSWORD\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3SvhzJ4GIM2"
      },
      "source": [
        "This instance, stored in the variable graph, can be used to interact with the Neo4j database. The Neo4jGraph class provides methods and functionalities to execute queries, retrieve data, and manipulate the graph database, facilitating seamless integration and operations within the LangChain framework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "XwzkR_GOzArj"
      },
      "outputs": [],
      "source": [
        "graph = Neo4jGraph()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw1cepLUGWEM"
      },
      "source": [
        "This code snippet loads web pages from a list of URLs using the `WebBaseLoader` class from the `langchain_community.document_loaders` module:\n",
        "\n",
        "\n",
        "- **WebBaseLoader**: This class is used for loading web pages from URLs provided in a list. It likely uses web scraping techniques to retrieve the content from each URL.\n",
        "- **loader.requests_kwargs**: This line sets a keyword argument for the HTTP request made by the loader. `{'verify': False}` disables SSL certificate verification, which can be useful when dealing with self-signed certificates or testing environments.\n",
        "- **data = loader.load()**: This line executes the loading process, fetching the HTML content from the specified URLs and storing it in the `data` variable.\n",
        "\n",
        "This approach is helpful for gathering content from multiple web pages for further processing, such as natural language processing or content analysis, within the LangChain framework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "nfpW5QoAudoX"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader([\n",
        "    \"https://www.geeksforgeeks.org/top-interview-questions-and-answers/\",\n",
        "    \"https://www.interviewbit.com/technical-interview-questions/\",\n",
        "    \"https://www.hackerrank.com/interview/interview-preparation-kit\",\n",
        "    \"https://leetcode.com/problemset/all/\"])\n",
        "\n",
        "# Load documents\n",
        "documents = loader.load()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First define or load your data\n",
        "# For example, if you're working with documents:\n",
        "from langchain.schema import Document\n",
        "data = [Document(page_content=\"This is some sample text\")]\n",
        "\n",
        "# Then you can access it\n",
        "print(len(data[0].page_content))  # This will now work"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VO2susfhYKK",
        "outputId": "35b717c0-da6f-4861-eab4-b7f3678e0a23"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4Fcg-eBv2AB",
        "outputId": "e6057448-6947-4e2b-e07d-f1c472b0958f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "len(data[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "fOWYO20aLVBL",
        "outputId": "73975d58-e117-4097-8bd1-027cbdf40e76"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This is some sample text'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "data[0].page_content[0:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNSNunP2Gd2c"
      },
      "source": [
        "\n",
        "\n",
        "- **TokenTextSplitter**: This class is used to split text into chunks, typically to prepare it for processing by language models or other natural language processing tasks.\n",
        "- **text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)**: Initializes an instance of `TokenTextSplitter` with a chunk size of 512 tokens and an overlap of 24 tokens between consecutive chunks.\n",
        "- **documents = text_splitter.split_documents(data)**: Splits the input `data` (presumably a collection of text documents) into chunks of the specified size and overlap, storing the resulting chunks in the `documents` variable.\n",
        "\n",
        "This approach is useful for breaking down large text documents or datasets into manageable pieces that can be processed more efficiently by downstream tasks or models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "WSBwTblIxmgO"
      },
      "outputs": [],
      "source": [
        "text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)\n",
        "documents = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total documents: {len(documents)}\")\n",
        "for i, doc in enumerate(documents):\n",
        "    print(f\"Document {i}: {doc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ik37oUmahtz4",
        "outputId": "079485ac-a984-4c36-d947-2a734f9284b2"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total documents: 1\n",
            "Document 0: page_content='This is some sample text'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "dI5DbzHLezVX",
        "outputId": "504e1a1e-f660-484e-98c0-04ae65701d18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='This is some sample text'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-36080ab1a237>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "print(documents[0])\n",
        "print(documents[1])\n",
        "print(documents[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2MEovPDGzBW"
      },
      "source": [
        "\n",
        "\n",
        "- **ChatOpenAI**: This class is used to interact with OpenAI's chat models. In this case, it's initialized with parameters like `temperature=0` and `model_name=\"gpt-3.5-turbo-0125\"`.\n",
        "- **LLMGraphTransformer**: This class is used to transform text documents into graph documents using a language model (LLM).\n",
        "- **Neo4jGraph**: This class represents a connection to a Neo4j graph database.\n",
        "\n",
        "\n",
        "The code first initializes the language model (`llm`) and the transformer (`llm_transformer`) to convert text documents (`documents`) into graph documents. Then, it initializes a `Neo4jGraph` instance (`graph`) and adds the transformed graph documents to the Neo4j database using the `add_graph_documents` method.\n",
        "\n",
        "This approach integrates natural language processing with graph database operations, enabling the creation of structured graph representations from unstructured text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DttkbLsyAza",
        "outputId": "7094615c-48c0-4a3c-9e8a-bb8d89e7d1fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py:1673: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo-0125 since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-0125\") # gpt-4-0125-preview occasionally has issues\n",
        "llm_transformer = LLMGraphTransformer(llm=llm)\n",
        "\n",
        "# Following needs to be used only first time to load the data. It took 3m 44s\n",
        "graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW1WQB5-PYhj"
      },
      "source": [
        "**The following needs to be run for the first time when you load data into DB.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "qxodjNISPSXK"
      },
      "outputs": [],
      "source": [
        "graph.add_graph_documents(\n",
        "    graph_documents,\n",
        "    baseEntityLabel=True,\n",
        "    include_source=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (documents[5])\n",
        "print(graph_documents[5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "ewmpld5Cvzvj",
        "outputId": "a95c6cc8-a363-4478-ee9f-f0c8803016de"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-c6c39cadeb5c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_documents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-76PrdloHXmK"
      },
      "source": [
        "The `showGraph` function takes an optional `Cypher `query string as input (defaulting to a predefined query), establishes a connection to `Neo4j` using credentials from environment variables, executes the query to retrieve graph data, and then visualizes it using a `GraphWidget`. The widget allows interactive exploration of the graph nodes and relationships directly within the notebook environment, making it easy to analyze and understand graph structures and connections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517,
          "referenced_widgets": [
            "a03e6c640d374f4e9c4a8ec49b8858bd",
            "165f9fa5a4c547ce943e68ba2eb1bd4e"
          ]
        },
        "id": "verAasH9yIu6",
        "outputId": "f14897e3-038e-4e56-fcda-f9f82843ac12"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "GraphWidget(layout=Layout(height='500px', width='100%'))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a03e6c640d374f4e9c4a8ec49b8858bd"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        }
      ],
      "source": [
        "# directly show the graph resulting from the given Cypher query\n",
        "default_cypher = \"MATCH (s)-[r:!MENTIONS]->(t) RETURN s,r,t LIMIT 50\"\n",
        "\n",
        "def showGraph(cypher: str = default_cypher):\n",
        "    # create a neo4j session to run queries\n",
        "    driver = GraphDatabase.driver(\n",
        "        uri = os.environ[\"NEO4J_URI\"],\n",
        "        auth = (os.environ[\"NEO4J_USERNAME\"],\n",
        "                os.environ[\"NEO4J_PASSWORD\"]))\n",
        "    session = driver.session()\n",
        "    widget = GraphWidget(graph = session.run(cypher).graph())\n",
        "    widget.node_label_mapping = 'id'\n",
        "    #display(widget)\n",
        "    return widget\n",
        "\n",
        "showGraph()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LSQjHkAHuTj"
      },
      "source": [
        "The `Neo4jVector.from_existing_graph()` method is used to create a vector index within a Neo4j database. It integrates embeddings from OpenAI's models, facilitating efficient search and retrieval operations based on both textual content and embedding similarities.\n",
        "\n",
        "### Key Components:\n",
        "\n",
        "- **LangChain Components**:\n",
        "  - **Neo4jVector**: This class from LangChain Community is designed to manage vector indexes within Neo4j, enabling storage and retrieval of embeddings associated with graph nodes.\n",
        "  - **OpenAIEmbeddings**: This class from LangChain OpenAI provides tools for generating embeddings using OpenAI's models.\n",
        "\n",
        "- **Initialization Parameters**:\n",
        "  - **OpenAIEmbeddings()**: Initializes the use of OpenAI's models to generate embeddings.\n",
        "  - **search_type=\"hybrid\"**: Specifies a hybrid search approach, leveraging both textual content and embedding similarities for efficient querying.\n",
        "  - **node_label=\"Document\"**: Specifies that nodes labeled \"Document\" in the Neo4j graph will store document-related information.\n",
        "  - **text_node_properties=[\"text\"]**: Defines the property name(s) within \"Document\" nodes where textual content is stored.\n",
        "  - **embedding_node_property=\"embedding\"**: Specifies the property name where embeddings are stored within the \"Document\" nodes.\n",
        "\n",
        "### Usage Scenario:\n",
        "\n",
        "This setup is useful in scenarios where you want to store and query documents based on both their textual content and their embeddings. For example, you can store documents in Neo4j, extract their embeddings using OpenAI's models, and then use this vector index to efficiently search for documents based on their semantic similarity to a given query or to other documents in the database.\n",
        "\n",
        "This approach leverages the capabilities of Neo4j for graph-based storage and retrieval, coupled with advanced embedding techniques from OpenAI, providing a powerful toolset for managing and analyzing textual data within a graph database environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "J23HXjaEzdu8"
      },
      "outputs": [],
      "source": [
        "vector_index = Neo4jVector.from_existing_graph(\n",
        "    OpenAIEmbeddings(),\n",
        "    search_type=\"hybrid\",\n",
        "    node_label=\"Document\",\n",
        "    text_node_properties=[\"text\"],\n",
        "    embedding_node_property=\"embedding\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U__qaE9sHtW8"
      },
      "source": [
        "\n",
        "\n",
        "### Retriever Setup:\n",
        "\n",
        "The code snippet performs the following actions:\n",
        "\n",
        "1. **Create Fulltext Index**:\n",
        "   - The `graph.query()` statement creates a fulltext index named `entity` if it does not already exist. This index is applied to nodes labeled `__Entity__` and indexes the `id` property of these nodes for efficient querying.\n",
        "\n",
        "2. **Entity Data Model**:\n",
        "   - The `Entities` class, derived from `BaseModel`, defines a structured representation for identifying information about entities extracted from text.\n",
        "   - **Names**: A list of strings representing entities such as courses, professors, requirements, or project entities found in the text.\n",
        "\n",
        "3. **Chat Prompt Template**:\n",
        "   - The `ChatPromptTemplate.from_messages()` method initializes a prompt template for interacting with users.\n",
        "   - The prompt consists of a system message informing the user about the extraction task and a human message specifying the format for input to extract information.\n",
        "\n",
        "4. **Entity Chain**:\n",
        "   - The `prompt | llm.with_structured_output(Entities)` constructs an entity extraction pipeline using LangChain:\n",
        "     - **Prompt**: Provides instructions to the user on how to input questions or text for entity extraction.\n",
        "     - **LLM (Language Model)**: Processes the input using OpenAI's language model to extract structured entities as defined by the `Entities` class.\n",
        "\n",
        "### Usage:\n",
        "\n",
        "This setup is designed to facilitate the extraction of entities from text input in a structured and systematic manner. The fulltext index ensures that entity nodes in the Neo4j graph are efficiently indexed, allowing for quick retrieval of relevant entities based on their `id` property. The `Entities` data model and the prompt template guide the interaction with users, ensuring that the extraction process is clear and consistent.\n",
        "\n",
        "This approach leverages natural language processing and graph database capabilities to manage and analyze textual data effectively, supporting tasks such as information extraction, entity linking, and knowledge base population within a broader application or system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DLAEMgHz2nF",
        "outputId": "1be11d83-2462-42d2-e683-310e0aca5e7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py:1660: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py:1673: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo-0125 since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Retriever\n",
        "\n",
        "graph.query(\n",
        "    \"CREATE FULLTEXT INDEX entity IF NOT EXISTS FOR (e:__Entity__) ON EACH [e.id]\")\n",
        "\n",
        "# Extract entities from text\n",
        "class Entities(BaseModel):\n",
        "    \"\"\"Identifying information about entities.\"\"\"\n",
        "\n",
        "    names: List[str] = Field(\n",
        "        ...,\n",
        "        description=\"All the courses, requirements, or business entities that \"\n",
        "        \"appear in the text\",\n",
        "    )\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are extracting course, professors, requirements, project entities from the text.\",\n",
        "        ),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Use the given format to extract information from the following \"\n",
        "            \"input: {question}\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "entity_chain = prompt | llm.with_structured_output(Entities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JKJNR3YWU-p"
      },
      "source": [
        "### Description\n",
        "\n",
        "This script is used to retrieve related information from a Neo4j graph database based on user input.\n",
        "\n",
        "#### Functions:\n",
        "\n",
        "- **`generate_full_text_query(input: str)`**  \n",
        "  Converts a given input string into a fuzzy full-text search query.  \n",
        "  - Splits the input into individual words.\n",
        "  - Appends a similarity threshold (`~2`) to each word to allow minor misspellings.\n",
        "  - Combines the words using the `AND` operator for more accurate matching.\n",
        "\n",
        "- **`structured_retriever(question: str)`**  \n",
        "  Extracts entities from a natural language question and fetches related nodes from the graph.  \n",
        "  - Uses `entity_chain` to extract key entity names from the question.\n",
        "  - For each entity, performs a full-text search using the generated query.\n",
        "  - Retrieves up to 50 paths showing relationships to and from each matching node.\n",
        "  - Returns the results as a formatted string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "V1jRiI6G0J6P"
      },
      "outputs": [],
      "source": [
        "def generate_full_text_query(input: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate a full-text search query for a given input string.\n",
        "\n",
        "    This function constructs a query string suitable for a full-text search.\n",
        "    It processes the input string by splitting it into words and appending a\n",
        "    similarity threshold (~2 changed characters) to each word, then combines\n",
        "    them using the AND operator. Useful for mapping entities from user questions\n",
        "    to database values, and allows for some misspelings.\n",
        "    \"\"\"\n",
        "    full_text_query = \"\"\n",
        "    words = [el for el in remove_lucene_chars(input).split() if el]\n",
        "    for word in words[:-1]:\n",
        "        full_text_query += f\" {word}~2 AND\"\n",
        "    full_text_query += f\" {words[-1]}~2\"\n",
        "    return full_text_query.strip()\n",
        "\n",
        "# Fulltext index query\n",
        "def structured_retriever(question: str) -> str:\n",
        "    \"\"\"\n",
        "    Collects the neighborhood of entities mentioned\n",
        "    in the question\n",
        "    \"\"\"\n",
        "    result = \"\"\n",
        "    entities = entity_chain.invoke({\"question\": question})\n",
        "    for entity in entities.names:\n",
        "        response = graph.query(\n",
        "            \"\"\"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\n",
        "            YIELD node,score\n",
        "            CALL {\n",
        "              WITH node\n",
        "              MATCH (node)-[r:!MENTIONS]->(neighbor)\n",
        "              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\n",
        "              UNION ALL\n",
        "              WITH node\n",
        "              MATCH (node)<-[r:!MENTIONS]-(neighbor)\n",
        "              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\n",
        "            }\n",
        "            RETURN output LIMIT 50\n",
        "            \"\"\",\n",
        "            {\"query\": generate_full_text_query(entity)},\n",
        "        )\n",
        "        result += \"\\n\".join([el['output'] for el in response])\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDRKabxJWU-q"
      },
      "source": [
        "- **`retriever(question: str)`**  \n",
        "  Combines results from both structured and unstructured data sources to answer a user question.  \n",
        "  - Logs the original search query.\n",
        "  - Uses `structured_retriever` to fetch entity-related information from a Neo4j graph (structured data).\n",
        "  - Performs a similarity search over a vector index to retrieve relevant documents (unstructured data).\n",
        "  - Formats and returns both data types in a combined response.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "C1ftDuLy0P8X"
      },
      "outputs": [],
      "source": [
        "def retriever(question: str):\n",
        "    print(f\"Search query: {question}\")\n",
        "    structured_data = structured_retriever(question)\n",
        "    unstructured_data = [el.page_content for el in vector_index.similarity_search(question)]\n",
        "    final_data = f\"\"\"Structured data:\n",
        "{structured_data}\n",
        "Unstructured data:\n",
        "{\"#Document \". join(unstructured_data)}\n",
        "    \"\"\"\n",
        "    return final_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_-vBRUQWU-r"
      },
      "source": [
        "### Condensing Chat History for Standalone Question Generation\n",
        "\n",
        "This code helps rephrase a follow-up question into a standalone question by considering the previous chat history.\n",
        "\n",
        "#### Components:\n",
        "\n",
        "- **`_template` and `CONDENSE_QUESTION_PROMPT`**  \n",
        "  Defines a prompt template used to generate a standalone question from a follow-up query and previous chat context.\n",
        "\n",
        "- **`_format_chat_history(chat_history: List[Tuple[str, str]]) -> List`**  \n",
        "  Converts a list of chat history (pairs of human and AI messages) into a format compatible with LLMs, using `HumanMessage` and `AIMessage` objects.\n",
        "\n",
        "- **`_search_query`**  \n",
        "  A conditional chain (`RunnableBranch`) that:\n",
        "  - If chat history exists:\n",
        "    - Formats the history.\n",
        "    - Fills the prompt with the history and the follow-up question.\n",
        "    - Sends it to an LLM (e.g., OpenAI) to generate a standalone question.\n",
        "  - If no chat history exists:\n",
        "    - Returns the original question as-is.\n",
        "\n",
        "This setup is typically used to improve retrieval or question answering pipelines by ensuring queries are self-contained.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "azltkyJN0U_I"
      },
      "outputs": [],
      "source": [
        "# Condense a chat history and follow-up question into a standalone question\n",
        "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question,\n",
        "in its original language.\n",
        "Chat History:\n",
        "{chat_history}\n",
        "Follow Up Input: {question}\n",
        "Standalone question:\"\"\"  # noqa: E501\n",
        "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
        "\n",
        "def _format_chat_history(chat_history: List[Tuple[str, str]]) -> List:\n",
        "    buffer = []\n",
        "    for human, ai in chat_history:\n",
        "        buffer.append(HumanMessage(content=human))\n",
        "        buffer.append(AIMessage(content=ai))\n",
        "    return buffer\n",
        "\n",
        "_search_query = RunnableBranch(\n",
        "    # If input includes chat_history, we condense it with the follow-up question\n",
        "    (\n",
        "        RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\n",
        "            run_name=\"HasChatHistoryCheck\"\n",
        "        ),  # Condense follow-up question and chat into a standalone_question\n",
        "        RunnablePassthrough.assign(\n",
        "            chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
        "        )\n",
        "        | CONDENSE_QUESTION_PROMPT\n",
        "        | ChatOpenAI(temperature=0)\n",
        "        | StrOutputParser(),\n",
        "    ),\n",
        "    # Else, we have no chat history, so just pass through the question\n",
        "    RunnableLambda(lambda x : x[\"question\"]),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpA9dhbRWU-6"
      },
      "source": [
        "### Answering a Question Using Context-Aware Retrieval\n",
        "\n",
        "This code defines a chain that answers a user question based only on retrieved context from structured and unstructured sources.\n",
        "\n",
        "#### Components:\n",
        "\n",
        "- **`template` and `prompt`**  \n",
        "  A prompt template that:\n",
        "  - Inserts the relevant context and the user's question.\n",
        "  - Instructs the model to respond in natural, concise language.\n",
        "\n",
        "- **`chain`**  \n",
        "  A composed pipeline that:\n",
        "  - Runs two parallel steps:\n",
        "    - **`_search_query | retriever`**: Uses the `_search_query` logic to condense the question (if chat history exists), and then fetches relevant context via the `retriever` function.\n",
        "    - **`RunnablePassthrough()`**: Passes the original question directly.\n",
        "  - Fills the `prompt` with the retrieved context and question.\n",
        "  - Sends the filled prompt to the language model (`llm`) to generate a response.\n",
        "  - Parses and returns the final answer as a plain string.\n",
        "\n",
        "This setup ensures that the model's response is grounded in actual retrieved knowledge, improving relevance and accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "Bru0C7ox0aJY"
      },
      "outputs": [],
      "source": [
        "# Answer the question based only on the following context\n",
        "template = \"\"\"Answer the query:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Use natural language and be concise.\n",
        "Answer:\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = (\n",
        "    RunnableParallel(\n",
        "        {\n",
        "            \"context\": _search_query | retriever,\n",
        "            \"question\": RunnablePassthrough(),\n",
        "        }\n",
        "    )\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87XK7HSgWU-8"
      },
      "source": [
        "### `chain.invoke({\"question\": \"...\"})` — What It Does\n",
        "\n",
        "This command executes a full pipeline that answers a user’s question by retrieving relevant data from both structured and unstructured sources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBOP7MYrV3Af"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "Xv4LdswWPvZe",
        "outputId": "43193286-8948-4890-bf89-7e9168cfe89c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search query: explain important concepts in java?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL (node, node) { ... }} {position: line: 3, column: 13, offset: 104} for query: \"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\\n            YIELD node,score\\n            CALL {\\n              WITH node\\n              MATCH (node)-[r:!MENTIONS]->(neighbor)\\n              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\\n              UNION ALL\\n              WITH node\\n              MATCH (node)<-[r:!MENTIONS]-(neighbor)\\n              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\\n            }\\n            RETURN output LIMIT 50\\n            \"\n",
            "WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL (node, node) { ... }} {position: line: 3, column: 13, offset: 104} for query: \"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\\n            YIELD node,score\\n            CALL {\\n              WITH node\\n              MATCH (node)-[r:!MENTIONS]->(neighbor)\\n              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\\n              UNION ALL\\n              WITH node\\n              MATCH (node)<-[r:!MENTIONS]-(neighbor)\\n              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\\n            }\\n            RETURN output LIMIT 50\\n            \"\n",
            "WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL () { ... }} {position: line: 1, column: 1, offset: 0} for query: \"CALL { CALL db.index.vector.queryNodes($index, $k, $embedding) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score UNION CALL db.index.fulltext.queryNodes($keyword_index, $query, {limit: $k}) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score } WITH node, max(score) AS score ORDER BY score DESC LIMIT $k RETURN reduce(str='', k IN ['text'] | str + '\\\\n' + k + ': ' + coalesce(node[k], '')) AS text, node {.*, `embedding`: Null, id: Null, `text`: Null} AS metadata, score\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Structured data refers to data that is organized in a predefined format, such as databases or spreadsheets, making it easily searchable and analyzable. \\n\\nUnstructured data, on the other hand, refers to data that does not have a predefined format, such as text documents, images, or videos, making it more difficult to search and analyze.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "chain.invoke({\"question\": \"explain important concepts in java?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "L5C9R3Pp0lrw",
        "outputId": "2f4367d9-e250-40b7-de8b-621cd21d3c81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search query: what is a array ?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL (node, node) { ... }} {position: line: 3, column: 13, offset: 104} for query: \"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\\n            YIELD node,score\\n            CALL {\\n              WITH node\\n              MATCH (node)-[r:!MENTIONS]->(neighbor)\\n              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\\n              UNION ALL\\n              WITH node\\n              MATCH (node)<-[r:!MENTIONS]-(neighbor)\\n              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\\n            }\\n            RETURN output LIMIT 50\\n            \"\n",
            "WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL () { ... }} {position: line: 1, column: 1, offset: 0} for query: \"CALL { CALL db.index.vector.queryNodes($index, $k, $embedding) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score UNION CALL db.index.fulltext.queryNodes($keyword_index, $query, {limit: $k}) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score } WITH node, max(score) AS score ORDER BY score DESC LIMIT $k RETURN reduce(str='', k IN ['text'] | str + '\\\\n' + k + ': ' + coalesce(node[k], '')) AS text, node {.*, `embedding`: Null, id: Null, `text`: Null} AS metadata, score\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Structured data is data that is organized in a predefined format, such as tables or databases, making it easily searchable and analyzable. \\n\\nUnstructured data is data that does not have a predefined format, such as text documents or social media posts, making it more difficult to search and analyze.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "#True Positive\n",
        "chain.invoke({\"question\": \"what is a array ?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en1XQ9DlWU-_"
      },
      "source": [
        "### Disabling Warnings and Setting Logging Levels for Neo4j\n",
        "\n",
        "This code snippet is used to **suppress warnings** and **set logging levels** for specific parts of the code, especially in the context of working with the Neo4j database.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "QAd4_yQDRQMv"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import logging\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.getLogger(\"neo4j.notifications\").setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "duilHxHs1Ac9",
        "outputId": "ce40d3c1-584d-4bbd-8560-664c9003941d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search query: give me topics in java ?\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Structured data: Java data types, Java classes, Java methods\\nUnstructured data: Java code snippets, Java documentation, Java forum discussions'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 88
        }
      ],
      "source": [
        "#True Negative\n",
        "chain.invoke({\"question\": \"give me topics in java ?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "S7CARN8C1IRB",
        "outputId": "3bbb02d1-e38b-4453-ca27-b4133eff6eaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search query: what is an array ?\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Structured data is data that is organized in a predefined format, such as tables or databases, making it easily searchable and analyzable. \\n\\nUnstructured data is data that does not have a predefined format, such as text documents or social media posts, making it more difficult to search and analyze.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "# False Negative\n",
        "chain.invoke({\"question\": \"what is an array ?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "FR1gMLPx1S0M",
        "outputId": "679c1696-5e7c-45a7-c814-dd089b01b67f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search query: what is a java ?\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Structured data refers to data that is organized in a predefined format, such as tables or databases, making it easily searchable and analyzable. \\n\\nUnstructured data, on the other hand, refers to data that does not have a predefined format, such as text documents or social media posts, making it more difficult to search and analyze.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "# False Positives\n",
        "chain.invoke({\"question\": \"what is a java ?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHVxM45pR1p5"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "DJRGvroZ1cSJ",
        "outputId": "bf141297-b21c-4f2e-e64b-f4c7966ef56f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search query: What is java program?\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Structured data refers to data that is organized in a predefined format, such as tables or databases, making it easily searchable and analyzable. \\n\\nUnstructured data, on the other hand, refers to data that does not have a predefined format, such as text documents or social media posts, making it more difficult to search and analyze.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 94
        }
      ],
      "source": [
        "chain.invoke({\"question\": \"What is java program?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "Q784P6LWt-ua",
        "outputId": "566235ed-09f0-41a4-b43d-6db066f6ea4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search query: What is the meaning of java ?\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Structured data refers to data that is organized in a specific format, such as tables or databases, making it easily searchable and analyzable. \\n\\nUnstructured data, on the other hand, refers to data that does not have a specific format or organization, such as text documents or social media posts, making it more difficult to analyze and extract insights from.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 96
        }
      ],
      "source": [
        "chain.invoke({\"question\": \"What is the meaning of java ?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "T1x2V51PuENT",
        "outputId": "f286f736-2a66-4a2e-c39b-89d8b466ce0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search query: What are the required course to learn java ?\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Structured data: The required courses to learn Java typically include Introduction to Java Programming, Object-Oriented Programming in Java, Data Structures and Algorithms in Java, and Advanced Java Programming.\\n\\nUnstructured data: There is no specific requirement for learning Java, but having a basic understanding of programming concepts and familiarity with object-oriented programming can be helpful.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "source": [
        "chain.invoke({\"question\": \"What are the required course to learn java ?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "EjgwVXmyvqmM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "86b00308-5f72-4b70-b29a-abdb8ed02c07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search query: Is fire hot tell me T or F?\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'True, fire is hot.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 98
        }
      ],
      "source": [
        "chain.invoke({\"question\": \"Is fire hot tell me T or F?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "EIkFhW5cWU_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "492294bd-7959-4123-fabd-e645b5af6abd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile main.py\n",
        "from langchain_core.runnables import (\n",
        "    RunnableBranch,\n",
        "    RunnableLambda,\n",
        "    RunnableParallel,\n",
        "    RunnablePassthrough,\n",
        ")\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.prompts.prompt import PromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from typing import Tuple, List, Optional\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import os\n",
        "from langchain_community.graphs import Neo4jGraph\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
        "from neo4j import GraphDatabase\n",
        "from langchain_community.vectorstores import Neo4jVector\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars\n",
        "from langchain_core.runnables import ConfigurableField, RunnableParallel, RunnablePassthrough\n",
        "\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  from google.colab import output\n",
        "  output.enable_custom_widget_manager()\n",
        "except:\n",
        "  pass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\" # replace your OPEN_API key\n",
        "os.environ[\"NEO4J_URI\"] = \"neo4j+s://2bb0b0b7.databases.neo4j.io\" # NEO4J_URI\n",
        "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\" # NEO4J_USERNAME\n",
        "os.environ[\"NEO4J_PASSWORD\"] = \"TeR_N6D_HCnxvYZxasyYyKDS4B8KotU7-YydeiF239Q\" # NEO4J_PASSWORD\n",
        "\n",
        "_search_query = RunnableLambda(lambda x : x[\"question\"])\n",
        "\n",
        "\n",
        "graph = Neo4jGraph()\n",
        "\n",
        "llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-0125\") # gpt-4-0125-preview occasionally has issues\n",
        "llm_transformer = LLMGraphTransformer(llm=llm)\n",
        "\n",
        "vector_index = Neo4jVector.from_existing_graph(\n",
        "    OpenAIEmbeddings(),\n",
        "    search_type=\"hybrid\",\n",
        "    node_label=\"Document\",\n",
        "    text_node_properties=[\"text\"],\n",
        "    embedding_node_property=\"embedding\"\n",
        ")\n",
        "\n",
        "\n",
        "graph.query(\n",
        "    \"CREATE FULLTEXT INDEX entity IF NOT EXISTS FOR (e:__Entity__) ON EACH [e.id]\")\n",
        "\n",
        "# Extract entities from text\n",
        "class Entities(BaseModel):\n",
        "    \"\"\"Identifying information about entities.\"\"\"\n",
        "\n",
        "    names: List[str] = Field(\n",
        "        ...,\n",
        "        description=\"All the courses, requirements, or business entities that \"\n",
        "        \"appear in the text\",\n",
        "    )\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are extracting course, professors, requirements, project entities from the text.\",\n",
        "        ),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Use the given format to extract information from the following \"\n",
        "            \"input: {question}\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "entity_chain = prompt | llm.with_structured_output(Entities)\n",
        "\n",
        "def generate_full_text_query(input: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate a full-text search query for a given input string.\n",
        "\n",
        "    This function constructs a query string suitable for a full-text search.\n",
        "    It processes the input string by splitting it into words and appending a\n",
        "    similarity threshold (~2 changed characters) to each word, then combines\n",
        "    them using the AND operator. Useful for mapping entities from user questions\n",
        "    to database values, and allows for some misspelings.\n",
        "    \"\"\"\n",
        "    full_text_query = \"\"\n",
        "    words = [el for el in remove_lucene_chars(input).split() if el]\n",
        "    for word in words[:-1]:\n",
        "        full_text_query += f\" {word}~2 AND\"\n",
        "    full_text_query += f\" {words[-1]}~2\"\n",
        "    return full_text_query.strip()\n",
        "\n",
        "# Fulltext index query\n",
        "def structured_retriever(question: str) -> str:\n",
        "    \"\"\"\n",
        "    Collects the neighborhood of entities mentioned\n",
        "    in the question\n",
        "    \"\"\"\n",
        "    result = \"\"\n",
        "    entities = entity_chain.invoke({\"question\": question})\n",
        "    for entity in entities.names:\n",
        "        response = graph.query(\n",
        "            \"\"\"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\n",
        "            YIELD node,score\n",
        "            CALL {\n",
        "              WITH node\n",
        "              MATCH (node)-[r:!MENTIONS]->(neighbor)\n",
        "              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\n",
        "              UNION ALL\n",
        "              WITH node\n",
        "              MATCH (node)<-[r:!MENTIONS]-(neighbor)\n",
        "              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\n",
        "            }\n",
        "            RETURN output LIMIT 50\n",
        "            \"\"\",\n",
        "            {\"query\": generate_full_text_query(entity)},\n",
        "        )\n",
        "        result += \"\\n\".join([el['output'] for el in response])\n",
        "    return result\n",
        "\n",
        "def retriever(question: str):\n",
        "    print(f\"Search query: {question}\")\n",
        "    structured_data = structured_retriever(question)\n",
        "    unstructured_data = [el.page_content for el in vector_index.similarity_search(question)]\n",
        "    final_data = f\"\"\" Structured data: {structured_data} Unstructured data: {\"#Document \". join(unstructured_data)}  \"\"\"\n",
        "    return final_data\n",
        "\n",
        "\n",
        "def answerquery(question: str):\n",
        "  template = \"\"\"Answer the question based only on the following context:\n",
        "  {context}\n",
        "\n",
        "  Question: {question}\n",
        "  Use natural language and be concise.\n",
        "  Answer:\"\"\"\n",
        "  prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "  chain = (\n",
        "      RunnableParallel(\n",
        "          {\n",
        "              \"context\": _search_query | retriever,\n",
        "              \"question\": RunnablePassthrough(),\n",
        "          }\n",
        "      )\n",
        "      | prompt\n",
        "      | llm\n",
        "      | StrOutputParser()\n",
        "  )\n",
        "\n",
        "  return chain.invoke(({\"question\": question}))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "UzFfyTKBZfoa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a1de045-bd79-4a12-99c0-a275aaece4c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from main import answerquery  # Import the function for answering queries\n",
        "\n",
        "# Set up the title of the app\n",
        "st.title(\"Interview Insight\")\n",
        "\n",
        "# Initialize chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# React to user input\n",
        "if prompt := st.text_input(\"Ask me anything about the java \"):\n",
        "    # Display user message in chat message container\n",
        "    st.chat_message(\"user\").markdown(prompt)\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    # Get response from the medical question-answering function\n",
        "    response = answerquery(prompt)\n",
        "\n",
        "    # Display assistant response in chat message container\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.markdown(response)\n",
        "    # Add assistant response to chat history\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFaxmm4cZyu9",
        "outputId": "62e6591b-de6d-4279-e0bd-f0e7edabdea7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.46.38.236:8501\u001b[0m\n",
            "\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a03e6c640d374f4e9c4a8ec49b8858bd": {
          "model_module": "yfiles-jupyter-graphs",
          "model_name": "GraphModel",
          "model_module_version": "^1.10.5",
          "state": {
            "_context_pane_mapping": [
              {
                "id": "Neighborhood",
                "title": "Neighborhood"
              },
              {
                "id": "Data",
                "title": "Data"
              },
              {
                "id": "Search",
                "title": "Search"
              },
              {
                "id": "About",
                "title": "About"
              }
            ],
            "_data_importer": "neo4j",
            "_directed": true,
            "_dom_classes": [],
            "_edges": [],
            "_graph_layout": {},
            "_highlight": [],
            "_license": {},
            "_model_module": "yfiles-jupyter-graphs",
            "_model_module_version": "^1.10.5",
            "_model_name": "GraphModel",
            "_neighborhood": {},
            "_nodes": [],
            "_overview": {
              "enabled": null,
              "overview_set": false
            },
            "_selected_graph": [
              [],
              []
            ],
            "_sidebar": {
              "enabled": false,
              "start_with": null
            },
            "_view_count": null,
            "_view_module": "yfiles-jupyter-graphs",
            "_view_module_version": "^1.10.5",
            "_view_name": "GraphView",
            "layout": "IPY_MODEL_165f9fa5a4c547ce943e68ba2eb1bd4e"
          }
        },
        "165f9fa5a4c547ce943e68ba2eb1bd4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "500px",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}